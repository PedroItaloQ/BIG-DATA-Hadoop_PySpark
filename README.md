# Big Data Pipeline com PySpark e Hadoop

Este projeto demonstra como construir um pipeline de Big Data usando PySpark e Hadoop.

## Estrutura do Projeto
- `config/`: Contém os arquivos de configuração do Hadoop e Spark.
- `data/`: Diretório para dados brutos.
- `output/`: Resultados processados.
- `notebooks/`: Notebooks Jupyter para análise exploratória.
- `scripts/`: Scripts PySpark para o pipeline de dados.
- `logs/`: Diretório de logs do Spark.
- `env/`: Arquivos de configuração do ambiente de desenvolvimento.
- `Dockerfile`: Arquivo Docker para configurar o ambiente.

## Como Executar
1. Configure o ambiente de desenvolvimento:
   ```bash
   source env/setup_env.sh